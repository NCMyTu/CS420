{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://github.com/abduulrahmankhalid/Real-Time-Violence-Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-14T12:18:42.117949Z",
     "iopub.status.busy": "2025-01-14T12:18:42.117113Z",
     "iopub.status.idle": "2025-01-14T12:18:45.442110Z",
     "shell.execute_reply": "2025-01-14T12:18:45.441411Z",
     "shell.execute_reply.started": "2025-01-14T12:18:42.117888Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3283/3818913170.py:11: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use(\"seaborn\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import tensorflow\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "%matplotlib inline\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Flatten, Input, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Lion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "# To Show a Video in Notebook\n",
    "def Play_Video(filepath):\n",
    "    html = ''\n",
    "    video = open(filepath,'rb').read()\n",
    "    src = 'data:video/mp4;base64,' + b64encode(video).decode()\n",
    "    html += '<video width=640 muted controls autoplay loop><source src=\"%s\" type=\"video/mp4\"></video>' % src \n",
    "    return HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes Directories\n",
    "NonViolnceVideos_Dir = \"../input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence/\"\n",
    "ViolnceVideos_Dir = \"../input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence/\"\n",
    "\n",
    "# Retrieve the list of all the video files present in the Class Directory.\n",
    "NonViolence_files_names_list = os.listdir(NonViolnceVideos_Dir)\n",
    "Violence_files_names_list = os.listdir(ViolnceVideos_Dir)\n",
    "\n",
    "# Randomly select a video file from the Classes Directory.\n",
    "Random_NonViolence_Video = random.choice(NonViolence_files_names_list)\n",
    "Random_Violence_Video = random.choice(Violence_files_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Play_Video(f\"{NonViolnceVideos_Dir}/{Random_NonViolence_Video}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Play_Video(f\"{ViolnceVideos_Dir}/{Random_Violence_Video}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT, IMAGE_WIDTH = 64, 64\n",
    "SEQUENCE_LENGTH = 16 # number of frames to be fed to model\n",
    "DATASET_DIR = \"/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset\"\n",
    "CLASSES_LIST = [\"NonViolence\",\"Violence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_extraction(video_path):\n",
    "    frames_list = []\n",
    "\n",
    "    video_reader = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n",
    " \n",
    "    for frame_counter in range(SEQUENCE_LENGTH):\n",
    "        # set frame position\n",
    "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
    "  \n",
    "        success, frame = video_reader.read() \n",
    " \n",
    "        if not success:\n",
    "            break\n",
    " \n",
    "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "        \n",
    "        normalized_frame = resized_frame / 255\n",
    "        \n",
    "        frames_list.append(normalized_frame)\n",
    " \n",
    "    video_reader.release()\n",
    " \n",
    "    return frames_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    features = []\n",
    "    labels = []\n",
    "    video_files_paths = []\n",
    "\n",
    "    for class_index, class_name in enumerate(CLASSES_LIST):\n",
    "        print(f'Extracting Data of Class: {class_name}')\n",
    "        \n",
    "        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))\n",
    "        \n",
    "        for file_name in files_list:\n",
    "            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)\n",
    " \n",
    "            frames = frames_extraction(video_file_path)\n",
    " \n",
    "            # check if the extracted frames are equal to the SEQUENCE_LENGTH specified.\n",
    "            # so ignore the vides having frames less than the SEQUENCE_LENGTH.\n",
    "            if len(frames) == SEQUENCE_LENGTH:\n",
    "                features.append(frames)\n",
    "                labels.append(class_index)\n",
    "                video_files_paths.append(video_file_path)\n",
    " \n",
    "    features = np.asarray(features)\n",
    "    labels = np.array(labels)  \n",
    "\n",
    "    return features, labels, video_files_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset.\n",
    "features, labels, video_files_paths = create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the extracted data\n",
    "np.save(\"features.npy\", features)\n",
    "np.save(\"labels.npy\", labels)\n",
    "np.save(\"video_files_paths.npy\", video_files_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T12:18:47.822374Z",
     "iopub.status.busy": "2025-01-14T12:18:47.821753Z",
     "iopub.status.idle": "2025-01-14T12:18:48.754598Z",
     "shell.execute_reply": "2025-01-14T12:18:48.753865Z",
     "shell.execute_reply.started": "2025-01-14T12:18:47.822339Z"
    }
   },
   "outputs": [],
   "source": [
    "features = np.load(\"/kaggle/input/video-feature-np/features.npy\")\n",
    "labels = np.load(\"/kaggle/input/video-feature-np/labels.npy\")\n",
    "video_files_paths = np.load(\"/kaggle/input/video-feature-np/video_files_paths.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T12:18:48.756520Z",
     "iopub.status.busy": "2025-01-14T12:18:48.756237Z",
     "iopub.status.idle": "2025-01-14T12:18:48.760832Z",
     "shell.execute_reply": "2025-01-14T12:18:48.759927Z",
     "shell.execute_reply.started": "2025-01-14T12:18:48.756493Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert labels into one-hot vectors\n",
    "encoded_labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T12:18:48.762393Z",
     "iopub.status.busy": "2025-01-14T12:18:48.762044Z",
     "iopub.status.idle": "2025-01-14T12:18:49.938497Z",
     "shell.execute_reply": "2025-01-14T12:18:49.937676Z",
     "shell.execute_reply.started": "2025-01-14T12:18:48.762356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 16, 64, 64, 3)\n",
      "(1400, 2)\n",
      "(300, 16, 64, 64, 3)\n",
      "(300, 2)\n",
      "(300, 16, 64, 64, 3)\n",
      "(300, 2)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_temp, y_train, y_temp = train_test_split(features, \n",
    "                                                    encoded_labels,\n",
    "                                                    stratify=encoded_labels,\n",
    "                                                    test_size=0.3,\n",
    "                                                    shuffle=True, \n",
    "                                                    random_state=2)\n",
    "\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_temp, \n",
    "                                                y_temp, \n",
    "                                                stratify=y_temp,\n",
    "                                                test_size=0.5,\n",
    "                                                shuffle=True, \n",
    "                                                random_state=2)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T12:18:51.922575Z",
     "iopub.status.busy": "2025-01-14T12:18:51.922229Z",
     "iopub.status.idle": "2025-01-14T12:18:51.929190Z",
     "shell.execute_reply": "2025-01-14T12:18:51.928283Z",
     "shell.execute_reply.started": "2025-01-14T12:18:51.922544Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    mobile_net = MobileNetV2(\n",
    "                            input_shape=(64, 64, 3),\n",
    "                            include_top=False,   \n",
    "                            weights='imagenet'\n",
    "                        )\n",
    "    # make last 40 layers trainable\n",
    "    mobile_net.trainable = True\n",
    "    for layer in mobile_net.layers[:-40]:\n",
    "      layer.trainable = False\n",
    "\n",
    "    lstm_fw = LSTM(units=32)\n",
    "    lstm_bw = LSTM(units=32, go_backwards = True)  \n",
    "    \n",
    "    model = Sequential(\n",
    "        [\n",
    "            Input(shape=(16, 64, 64, 3)),\n",
    "            TimeDistributed(mobile_net),\n",
    "            Dropout(0.25),             \n",
    "            TimeDistributed(Flatten()),\n",
    "            Bidirectional(lstm_fw, backward_layer = lstm_bw),\n",
    "            Dropout(0.25),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dropout(0.25),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.25),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.25),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.25),\n",
    "            Dense(2, activation ='softmax')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T12:18:54.418904Z",
     "iopub.status.busy": "2025-01-14T12:18:54.418557Z",
     "iopub.status.idle": "2025-01-14T12:18:55.964009Z",
     "shell.execute_reply": "2025-01-14T12:18:55.963078Z",
     "shell.execute_reply.started": "2025-01-14T12:18:54.418873Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3283/809978433.py:2: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  mobile_net = MobileNetV2(\n"
     ]
    }
   ],
   "source": [
    "MoBiLSTM_model = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T12:18:55.965830Z",
     "iopub.status.busy": "2025-01-14T12:18:55.965552Z",
     "iopub.status.idle": "2025-01-14T12:39:08.884160Z",
     "shell.execute_reply": "2025-01-14T12:39:08.883380Z",
     "shell.execute_reply.started": "2025-01-14T12:18:55.965804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 239ms/step - accuracy: 0.4927 - loss: 0.7044 - val_accuracy: 0.4600 - val_loss: 0.6946\n",
      "Epoch 2/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 146ms/step - accuracy: 0.4926 - loss: 0.7070 - val_accuracy: 0.5000 - val_loss: 0.6904\n",
      "Epoch 3/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 146ms/step - accuracy: 0.5163 - loss: 0.6968 - val_accuracy: 0.5300 - val_loss: 0.6906\n",
      "Epoch 4/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.4639 - loss: 0.7037 - val_accuracy: 0.5300 - val_loss: 0.6888\n",
      "Epoch 5/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.5470 - loss: 0.6876 - val_accuracy: 0.5700 - val_loss: 0.6861\n",
      "Epoch 6/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.5054 - loss: 0.6914 - val_accuracy: 0.5533 - val_loss: 0.6851\n",
      "Epoch 7/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.4858 - loss: 0.6967 - val_accuracy: 0.5667 - val_loss: 0.6844\n",
      "Epoch 8/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.5483 - loss: 0.6829 - val_accuracy: 0.6167 - val_loss: 0.6823\n",
      "Epoch 9/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 144ms/step - accuracy: 0.5746 - loss: 0.6825 - val_accuracy: 0.6133 - val_loss: 0.6808\n",
      "Epoch 10/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.5564 - loss: 0.6804 - val_accuracy: 0.6100 - val_loss: 0.6814\n",
      "Epoch 11/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 144ms/step - accuracy: 0.5723 - loss: 0.6809 - val_accuracy: 0.6233 - val_loss: 0.6762\n",
      "Epoch 12/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.5567 - loss: 0.6796 - val_accuracy: 0.6700 - val_loss: 0.6666\n",
      "Epoch 13/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 146ms/step - accuracy: 0.5589 - loss: 0.6793 - val_accuracy: 0.7067 - val_loss: 0.6566\n",
      "Epoch 14/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.5751 - loss: 0.6717 - val_accuracy: 0.7100 - val_loss: 0.6484\n",
      "Epoch 15/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.6241 - loss: 0.6540 - val_accuracy: 0.7033 - val_loss: 0.6390\n",
      "Epoch 16/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.6284 - loss: 0.6542 - val_accuracy: 0.7133 - val_loss: 0.6217\n",
      "Epoch 17/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.6394 - loss: 0.6415 - val_accuracy: 0.7600 - val_loss: 0.5973\n",
      "Epoch 18/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.6723 - loss: 0.6142 - val_accuracy: 0.7800 - val_loss: 0.5717\n",
      "Epoch 19/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.6838 - loss: 0.6126 - val_accuracy: 0.7733 - val_loss: 0.5450\n",
      "Epoch 20/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.7385 - loss: 0.5664 - val_accuracy: 0.8000 - val_loss: 0.5037\n",
      "Epoch 21/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.7708 - loss: 0.5219 - val_accuracy: 0.8100 - val_loss: 0.4746\n",
      "Epoch 22/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.7987 - loss: 0.4881 - val_accuracy: 0.8167 - val_loss: 0.4377\n",
      "Epoch 23/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.8221 - loss: 0.4641 - val_accuracy: 0.8367 - val_loss: 0.3999\n",
      "Epoch 24/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 146ms/step - accuracy: 0.8493 - loss: 0.4209 - val_accuracy: 0.8533 - val_loss: 0.3637\n",
      "Epoch 25/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.8657 - loss: 0.3880 - val_accuracy: 0.8667 - val_loss: 0.3465\n",
      "Epoch 26/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 144ms/step - accuracy: 0.8520 - loss: 0.3775 - val_accuracy: 0.8533 - val_loss: 0.3515\n",
      "Epoch 27/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.8774 - loss: 0.3500 - val_accuracy: 0.8667 - val_loss: 0.3232\n",
      "Epoch 28/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 146ms/step - accuracy: 0.8978 - loss: 0.2936 - val_accuracy: 0.8700 - val_loss: 0.3160\n",
      "Epoch 29/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.8804 - loss: 0.3233 - val_accuracy: 0.8900 - val_loss: 0.2838\n",
      "Epoch 30/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.9286 - loss: 0.2433 - val_accuracy: 0.8700 - val_loss: 0.3100\n",
      "Epoch 31/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 144ms/step - accuracy: 0.9160 - loss: 0.2596 - val_accuracy: 0.8700 - val_loss: 0.3036\n",
      "Epoch 32/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 144ms/step - accuracy: 0.9149 - loss: 0.2409 - val_accuracy: 0.8700 - val_loss: 0.3295\n",
      "Epoch 33/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.8930 - loss: 0.2939 - val_accuracy: 0.8933 - val_loss: 0.2684\n",
      "Epoch 34/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.9396 - loss: 0.1841 - val_accuracy: 0.9067 - val_loss: 0.2747\n",
      "Epoch 35/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.9373 - loss: 0.2073 - val_accuracy: 0.8933 - val_loss: 0.2981\n",
      "Epoch 36/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.9251 - loss: 0.2103 - val_accuracy: 0.9067 - val_loss: 0.2824\n",
      "Epoch 37/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 144ms/step - accuracy: 0.9578 - loss: 0.1539 - val_accuracy: 0.9000 - val_loss: 0.2979\n",
      "Epoch 38/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.9624 - loss: 0.1374 - val_accuracy: 0.8867 - val_loss: 0.3120\n",
      "Epoch 39/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.9503 - loss: 0.1564 - val_accuracy: 0.8800 - val_loss: 0.3445\n",
      "Epoch 40/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.9496 - loss: 0.1635 - val_accuracy: 0.9033 - val_loss: 0.2973\n",
      "Epoch 41/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.9557 - loss: 0.1517 - val_accuracy: 0.8933 - val_loss: 0.3135\n",
      "Epoch 42/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 144ms/step - accuracy: 0.9625 - loss: 0.1118 - val_accuracy: 0.9067 - val_loss: 0.2891\n",
      "Epoch 43/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.9626 - loss: 0.1463 - val_accuracy: 0.8867 - val_loss: 0.3196\n",
      "Epoch 44/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 145ms/step - accuracy: 0.9718 - loss: 0.1067 - val_accuracy: 0.9033 - val_loss: 0.2855\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor=\"val_accuracy\", \n",
    "                                patience=10, \n",
    "                                restore_best_weights=True)\n",
    "\n",
    "MoBiLSTM_model.compile(loss=\"categorical_crossentropy\",\n",
    "                       optimizer=SGD(learning_rate=0.001), \n",
    "                       metrics=[\"accuracy\"])\n",
    " \n",
    "# Fitting the model \n",
    "MobBiLSTM_model_history = MoBiLSTM_model.fit(x=x_train, \n",
    "                                             y=y_train,\n",
    "                                             epochs=100,\n",
    "                                             batch_size=8 ,\n",
    "                                             shuffle=True, \n",
    "                                             validation_data=(x_val, y_val), \n",
    "                                             callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T12:39:08.886176Z",
     "iopub.status.busy": "2025-01-14T12:39:08.885872Z",
     "iopub.status.idle": "2025-01-14T12:39:11.334462Z",
     "shell.execute_reply": "2025-01-14T12:39:11.333539Z",
     "shell.execute_reply.started": "2025-01-14T12:39:08.886148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 142ms/step - accuracy: 0.8989 - loss: 0.3134\n",
      "0.8933\n"
     ]
    }
   ],
   "source": [
    "acc = MoBiLSTM_model.evaluate(x_test, y_test)[1]\n",
    "print(f\"{acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MoBiLSTM_model.save(\"cnn_lstm.keras\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 176381,
     "sourceId": 397693,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6320476,
     "sourceId": 10230880,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
